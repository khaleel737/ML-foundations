{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "batch-regression-gradient.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khaleel737/ML-foundations/blob/master/notebooks/batch-regression-gradient.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1eOuVn9NGmF"
      },
      "source": [
        "# Gradient of Cost on a Batch of Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_KkT6RFNGmG"
      },
      "source": [
        "In this notebook, we expand on the partial derivative calculus of the [*Single Point Regression Gradient* notebook](https://github.com/jonkrohn/ML-foundations/blob/master/notebooks/single-point-regression-gradient.ipynb) to:\n",
        "\n",
        "* Calculate the gradient of mean squared error on a batch of data\n",
        "* Visualize gradient descent in action"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjicYQ5zNGmG"
      },
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIaLtIj9NGmK"
      },
      "source": [
        "xs = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7.])\n",
        "ys = torch.tensor([1.86, 1.31, .62, .33, .09, -.67, -1.23, -1.37])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpwZa7ezNGmN"
      },
      "source": [
        "def regression(my_x, my_m, my_b):\n",
        "    return my_m*my_x + my_b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HYqkV5_NGmP"
      },
      "source": [
        "m = torch.tensor([0.9]).requires_grad_()\n",
        "b = torch.tensor([0.1]).requires_grad_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gM5qQwd_NGmS"
      },
      "source": [
        "**Step 1**: Forward pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IRq0BEENGmS"
      },
      "source": [
        "yhats = regression(xs, m, b)\n",
        "yhats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MA8FiSK5NGmV"
      },
      "source": [
        "**Step 2**: Compare $\\hat{y}$ with true $y$ to calculate cost $C$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIUzVfyIiFZ4"
      },
      "source": [
        "As in the [*Regression in PyTorch* notebook](https://github.com/jonkrohn/ML-foundations/blob/master/notebooks/regression-in-pytorch.ipynb), let's use mean squared error, which averages quadratic cost across multiple data points: $$C = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y_i}-y_i)^2 $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXhJySbJNGmW"
      },
      "source": [
        "def mse(my_yhat, my_y):\n",
        "    sigma = torch.sum((my_yhat - my_y)**2)\n",
        "    return sigma/len(my_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JusNX1J4NGmZ"
      },
      "source": [
        "C = mse(yhats, ys)\n",
        "C"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDlyFxhFNGmb"
      },
      "source": [
        "**Step 3**: Use autodiff to calculate gradient of $C$ w.r.t. parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1awywt0FNGmb"
      },
      "source": [
        "C.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8J1h-BsNGmd"
      },
      "source": [
        "m.grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QVgiEt3NGmf"
      },
      "source": [
        "b.grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymYCctQhNGmh"
      },
      "source": [
        "**Return to *Calculus II* slides here to derive $\\frac{\\partial C}{\\partial m}$ and $\\frac{\\partial C}{\\partial b}$.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jki72o15NGmi"
      },
      "source": [
        "$$ \\frac{\\partial C}{\\partial m} = \\frac{2}{n} \\sum (\\hat{y}_i - y_i) \\cdot x_i $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3Nfrc2LNGmi"
      },
      "source": [
        "2*1/len(ys)*torch.sum((yhats - ys)*xs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuHlavWcNGmk"
      },
      "source": [
        "$$ \\frac{\\partial C}{\\partial b} = \\frac{2}{n} \\sum (\\hat{y}_i - y_i) $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55hFPIYqNGmk"
      },
      "source": [
        "2*1/len(ys)*torch.sum(yhats - ys)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_Rgoj4eNGmn"
      },
      "source": [
        "We don't need to explicitly create a standalone $\\nabla C$ object (Greek inverted delta is called *nabla* for \"harp\" but w.r.t. gradient is *del* as in \"del C\") for the remainder of the code in this notebook to run, but let's create it for fun now anyway and we'll make use of it in a later, related notebook:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1t69ayvtNGmn"
      },
      "source": [
        "gradient = torch.tensor([[b.grad.item(), m.grad.item()]]).T\n",
        "gradient"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOeN-r0jNGmp"
      },
      "source": [
        "Let's visualize the most pertinent metrics in a single plot:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ha6l7NFyNGmp"
      },
      "source": [
        "def labeled_regression_plot(my_x, my_y, my_m, my_b, my_C, include_grad=True):\n",
        "\n",
        "    title = 'Cost = {}'.format('%.3g' % my_C.item())\n",
        "    if include_grad:\n",
        "        xlabel = 'm = {}, m grad = {}'.format('%.3g' % my_m.item(), '%.3g' % my_m.grad.item())\n",
        "        ylabel = 'b = {}, b grad = {}'.format('%.3g' % my_b.item(), '%.3g' % my_b.grad.item())\n",
        "    else:\n",
        "        xlabel = 'm = {}'.format('%.3g' % my_m.item())\n",
        "        ylabel = 'b = {}'.format('%.3g' % my_b.item())\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.xlabel(xlabel)\n",
        "\n",
        "    ax.scatter(my_x, my_y, zorder=3)\n",
        "\n",
        "    x_min, x_max = ax.get_xlim()\n",
        "    y_min = regression(x_min, my_m, my_b).detach().item()\n",
        "    y_max = regression(x_max, my_m, my_b).detach().item()\n",
        "\n",
        "    ax.set_xlim([x_min, x_max])\n",
        "    _ = ax.plot([x_min, x_max], [y_min, y_max], c='C01')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMQLV5gdNGmr"
      },
      "source": [
        "labeled_regression_plot(xs, ys, m, b, C)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qv9b6XhbNGmt"
      },
      "source": [
        "**Step 4**: Gradient descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZafX4G7TNGmt"
      },
      "source": [
        "$\\frac{\\partial C}{\\partial m} = 36.3$ indicates that an increase in $m$ corresponds to a large increase in $C$.\n",
        "\n",
        "Meanwhile, $\\frac{\\partial C}{\\partial b} = 6.26$ indicates that an increase in $b$ also corresponds to an increase in $C$, though much less so than $m$.\n",
        "\n",
        "In the first round of training, the lowest hanging fruit with respect to reducing cost $C$ is therefore to decrease the slope of the regression line, $m$. There will also be a relatively small decrease in the $y$-intercept of the line, $b$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2LMBXARNGmu"
      },
      "source": [
        "optimizer = torch.optim.SGD([m, b], lr=0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "je7T1wxKNGmw"
      },
      "source": [
        "optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fz6J95xcNGmy"
      },
      "source": [
        "C = mse(regression(xs, m, b), ys)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGFSG151NGm1"
      },
      "source": [
        "labeled_regression_plot(xs, ys, m, b, C, include_grad=False) # Gradient of C hasn't been recalculated"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdbwrKZRNGm3"
      },
      "source": [
        "### Rinse and Repeat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhbOcYJXNGm3"
      },
      "source": [
        "Observe further rounds of training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDs2YfGxNGm4"
      },
      "source": [
        "epochs = 8\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    optimizer.zero_grad() # Reset gradients to zero; else they accumulate\n",
        "\n",
        "    yhats = regression(xs, m, b) # Step 1\n",
        "    C = mse(yhats, ys) # Step 2\n",
        "\n",
        "    C.backward() # Step 3\n",
        "\n",
        "    labeled_regression_plot(xs, ys, m, b, C)\n",
        "\n",
        "    optimizer.step() # Step 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epN8T_IoNGm5"
      },
      "source": [
        "In later rounds of training, after the model's slope $m$ has become closer to the slope represented by the data, $\\frac{\\partial C}{\\partial b}$ becomes negative, indicating an inverse relationship between $b$ and $C$. Meanwhile, $\\frac{\\partial C}{\\partial m}$ remains positive.\n",
        "\n",
        "This combination directs gradient descent to simultaneously adjust the $y$-intercept $b$ upwards and the slope $m$ downwards in order to reduce cost $C$ and, ultimately, fit the regression line snugly to the data."
      ]
    }
  ]
}